{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_md\")\n",
    "doc = Doc(nlp.vocab).from_disk(\"articles_clean_vectors.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Грамматические омонимы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем искать слова различных частей речи. Также исключим иностранные и с тегами `NUM` и `PROPN`. С PROPN очень много неверно размеченных слов, опять-таки из-за аббревиатур."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4524841/4524841 [00:06<00:00, 694309.78it/s]\n"
     ]
    }
   ],
   "source": [
    "data = defaultdict(lambda: defaultdict(set))\n",
    "\n",
    "for token in tqdm(doc):\n",
    "    if token.is_stop or not token.is_alpha:\n",
    "        continue\n",
    "    if token.morph.get(\"Foreign\") == [\"Yes\"]:\n",
    "        continue\n",
    "    if token.pos_ in [\"PROPN\", \"NUM\"]:\n",
    "        continue\n",
    "    data[token.text][token.pos_].add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 10\n",
    "filename = \"examples/grammar_hom.txt\"\n",
    "with open(filename, \"w\") as f:\n",
    "    for word, pos_data in data.items():\n",
    "        if len(pos_data) < 2:\n",
    "            continue\n",
    "        print(f\"Word: {word}\", file=f)\n",
    "        \n",
    "        for pos, tokens in pos_data.items():\n",
    "            print(f\"{word} ({pos})\", file=f)\n",
    "            for i, token in enumerate(tokens):\n",
    "                if i >= limit:\n",
    "                    break\n",
    "                print(token.sent.text, file=f)\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В файле `examples/grammar_hom.txt` лежат примеры слов. Но большая часть -- неправильные. В основном из-за неправильно определенной части речи.\n",
    "\n",
    "Немного вручную отобранных примеров в `examples/grammar_hom_manual.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эллипсис"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "203622it [00:01, 130977.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ellipsis_phrases = []\n",
    "for sent in tqdm(doc.sents):\n",
    "    for token in sent:\n",
    "        if token.dep_ == \"conj\" and token.head.dep_ == \"ROOT\":\n",
    "            ellipsis_phrases.append(sent)\n",
    "\n",
    "print(len(ellipsis_phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"examples/ellipsis.txt\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    for sent in ellipsis_phrases:\n",
    "        print(sent.text, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "С другой стороны, и цены Realme 6/Pro — как у индийских флагманов.\n",
      "Так что на новинки молодого китайского бренда стоит обратить самое пристальное внимание, и в первую очередь — на самую технически мощную модель, Realme 6 Pro.\n",
      "Корпус имеет совсем простую обтекаемую форму, плоской рамки по боковому периметру нет, по сути это единый пластиковый кожух, склеенный со стеклом экрана.\n",
      "Если надеть комплектный гибкий темный чехол, то удерживаться смартфон будет еще лучше, но в габаритах прибавит изрядно.\n",
      "Камеры сильно выступают за пределы поверхности, поэтому на столе смартфон лежит неустойчиво.\n",
      "Любопытно, что даже не будучи истинным флагманом, новинка получила продвинутую двойную камеру спереди.\n",
      "Правда, цвет проявляется лишь в градиентном узоре на задней панели, весь остальной корпус, по сути, черный.\n",
      "Ну и, как обычно, до нас самые яркие цвета почему-то не доходят, оранжевый на российский рынок не поставляется.\n",
      "Физические размеры экрана составляют 69×153 мм, соотношение сторон — 20:9, плотность точек — 399 ppi.\n",
      "Ширина рамки вокруг экрана составляет по 3 мм с боков, 4 мм сверху и 7 мм снизу — по современным меркам рамка достаточно широкая.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(ellipsis_phrases[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Весь вывод в `examples/ellipsis.txt`.\n",
    "\n",
    "Здесь уже большая часть предложений определена верно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лексические омонимы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код дальше создаст файлы с предложениями и словами. Эти файлы будут использоваться в другом ноутубке (`lexical_hom.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_sent = 5000\n",
    "filename_sents = \"lexical/sents.txt\"\n",
    "filename_words = \"lexical/words.txt\"\n",
    "\n",
    "words = defaultdict(int)\n",
    "for sent in list(doc.sents)[:cnt_sent]:\n",
    "    for token in sent:\n",
    "        if not token.is_alpha or token.is_stop:\n",
    "            continue\n",
    "        if token.morph.get(\"Foreign\") == [\"Yes\"]:\n",
    "            continue\n",
    "        if token.pos_ in [\"PROPN\"]:\n",
    "            continue\n",
    "        \n",
    "        words[token.text.lower()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename_sents, \"w\") as f:\n",
    "    for sent in list(doc.sents)[:cnt_sent]:\n",
    "        print(sent.text.lower(), file=f)\n",
    "        \n",
    "threshold = 3\n",
    "with open(filename_words, \"w\") as f:\n",
    "    for word, cnt in words.items():\n",
    "        if cnt > 5:\n",
    "            print(word, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Референциальные цепочки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут возникли большие трудности. Действительно долго искал и не смог найти работающей модели для русского языка. Для английского неплохие общедоступные модели существуют уже несколько лет. А для русского языка модели, которые я нашел, просто не получилось запустить. \n",
    "\n",
    "Я так понял, что относительно недавно появился размеченный корпус RuCoCo для задачи coreference resolution, и у некоторых библиотек (Spacy, в частности) в экспериментальных функциях есть заготовки для обучения моделей для распознавания цепочек. Так что в ближайшее время может появиться модели с более простым интерфейсом."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
